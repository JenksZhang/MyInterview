# 数据库面试知识点

## 📌 事务与ACID（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **ACID** | **原子性** | **一致性** | **隔离性** | **持久性** | **事务隔离级别** | **脏读** | **幻读** | **不可重复读**

### ✅ ACID详解

#### A - 原子性（Atomicity）

**定义：**事务是最小的执行单位，不允许分割。事务的所有操作要么全部完成，要么全部不完成。

**实现原理：**
- 通过 **undo log（回滚日志）** 实现
- 当事务回滚时，根据undo log进行反向操作
- 例如：INSERT的逆操作是DELETE，UPDATE的逆操作是反向UPDATE

**例子：**
```sql
-- 转账操作必须是原子的
BEGIN;
UPDATE account SET balance = balance - 100 WHERE id = 1;  -- A账户-100
UPDATE account SET balance = balance + 100 WHERE id = 2;  -- B账户+100
COMMIT;

-- 如果中间出错，两个UPDATE要么都执行，要么都不执行
```

#### C - 一致性（Consistency）

**定义：**执行事务前后，数据保持一致性状态，数据的完整性约束没有被破坏。

**实现原理：**
- 由应用程序和数据库共同保证
- 数据库：通过约束（主键、外键、唯一索引、CHECK约束）
- 应用程序：业务逻辑保证（如转账前后总金额不变）

**例子：**
```sql
-- 转账前后，总金额必须保持不变
-- 转账前：A=1000, B=500, 总额=1500
-- 转账后：A=900, B=600, 总额=1500 ✅
```

#### I - 隔离性（Isolation）

**定义：**并发执行的各个事务之间不能互相干扰。

**实现原理：**
- 通过 **锁机制** 和 **MVCC（多版本并发控制）** 实现
- 锁机制：读锁（共享锁）、写锁（排他锁）
- MVCC：保存数据的多个版本，不同事务看到不同版本

**隔离级别：**

| 隔离级别 | 脏读 | 不可重复读 | 幻读 | 说明 |
|---------|------|----------|------|------|
| **读未提交（READ UNCOMMITTED）** | ✅ | ✅ | ✅ | 可以读到未提交的数据 |
| **读已提交（READ COMMITTED）** | ❌ | ✅ | ✅ | 只能读到已提交的数据 |
| **可重复读（REPEATABLE READ）** | ❌ | ❌ | ✅ | 同一事务内多次读取结果一致 |
| **串行化（SERIALIZABLE）** | ❌ | ❌ | ❌ | 完全串行执行 |

**问题详解：**

**1. 脏读（Dirty Read）：**
```sql
-- 事务A
BEGIN;
UPDATE account SET balance = 1000 WHERE id = 1;
-- 此时未提交

-- 事务B
SELECT balance FROM account WHERE id = 1;  -- 读到1000（脏数据）

-- 事务A
ROLLBACK;  -- 事务A回滚了！

-- 事务B读到的数据是错误的
```

**2. 不可重复读（Non-Repeatable Read）：**
```sql
-- 事务A
BEGIN;
SELECT balance FROM account WHERE id = 1;  -- 读到500

-- 事务B
UPDATE account SET balance = 1000 WHERE id = 1;
COMMIT;

-- 事务A
SELECT balance FROM account WHERE id = 1;  -- 读到1000
-- 同一事务内，两次读取结果不一样
```

**3. 幻读（Phantom Read）：**
```sql
-- 事务A
BEGIN;
SELECT COUNT(*) FROM account WHERE balance > 100;  -- 读到5条

-- 事务B
INSERT INTO account (balance) VALUES (200);
COMMIT;

-- 事务A
SELECT COUNT(*) FROM account WHERE balance > 100;  -- 读到6条
-- 像出现了幻觉一样，多了一条记录
```

#### D - 持久性（Durability）

**定义：**一个事务被提交后，对数据库的改变是永久的，即使系统崩溃也不会丢失。

**实现原理：**
- 通过 **redo log（重做日志）** 实现
- 事务提交时，先写redo log，再修改数据页
- 即使系统崩溃，重启后也可以根据redo log恢复数据

**WAL（Write-Ahead Logging）：**
- 先写日志，再写磁盘
- 保证数据不丢失

### 🔥 面试追问点

#### 1️⃣ MySQL默认隔离级别是什么？为什么？（⭐⭐⭐⭐⭐）

**答案：可重复读（REPEATABLE READ）**

**原因：**
- 避免脏读和不可重复读
- 性能和一致性的权衡
- InnoDB通过MVCC + Next-Key Lock解决了大部分幻读问题

**查看和设置隔离级别：**
```sql
-- 查看当前隔离级别
SELECT @@transaction_isolation;

-- 设置隔离级别
SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;
```

#### 2️⃣ MVCC是什么？如何实现的？（⭐⭐⭐⭐⭐ 高频）

**MVCC（Multi-Version Concurrency Control，多版本并发控制）**

**核心思想：**
- 不加锁，通过保存数据的多个版本来实现并发控制
- 读不加锁，写不阻塞读
- 提高并发性能

**实现原理：**

**1. 隐藏字段：**
每行记录都有三个隐藏字段：
- `DB_TRX_ID`：最近修改事务ID
- `DB_ROLL_PTR`：回滚指针，指向undo log
- `DB_ROW_ID`：隐藏主键（如果没有主键）

**2. undo log版本链：**
```
当前数据：name='张三', age=25, DB_TRX_ID=100

undo log版本链：
[name='张三', age=25, DB_TRX_ID=100]
  ↓ DB_ROLL_PTR
[name='张三', age=24, DB_TRX_ID=90]
  ↓ DB_ROLL_PTR
[name='李四', age=24, DB_TRX_ID=80]
```

**3. Read View（读视图）：**
- 事务开始时创建Read View
- Read View记录了当前活跃的事务ID列表
- 根据Read View判断哪个版本的数据可见

**可见性判断规则：**
```cpp
// 伪代码
bool isVisible(DB_TRX_ID trx_id, ReadView read_view) {
    if (trx_id < read_view.min_trx_id) {
        return true;  // 事务已提交，可见
    }
    if (trx_id >= read_view.max_trx_id) {
        return false;  // 事务在Read View创建之后开始，不可见
    }
    if (trx_id in read_view.active_trx_ids) {
        return false;  // 事务还未提交，不可见
    }
    return true;  // 事务已提交，可见
}
```

**4. RC和RR的区别：**
- **READ COMMITTED**：每次SELECT都生成新的Read View
- **REPEATABLE READ**：事务开始时生成Read View，之后一直使用

#### 3️⃣ 如何解决幻读？（⭐⭐⭐⭐⭐）

**MySQL InnoDB解决方案：MVCC + Next-Key Lock**

**1. MVCC：**
- 快照读（普通SELECT）不会出现幻读
- 因为Read View是固定的，新插入的数据不可见

**2. Next-Key Lock（临键锁）：**
- 当前读（SELECT ... FOR UPDATE）使用Next-Key Lock
- Next-Key Lock = Record Lock（记录锁） + Gap Lock（间隙锁）

**例子：**
```sql
-- 表数据：id = 1, 5, 10
BEGIN;

-- 当前读，会加Next-Key Lock
SELECT * FROM t WHERE id > 5 FOR UPDATE;

-- 锁定范围：
-- (5, 10] 记录锁 + (10, +∞) 间隙锁
-- 其他事务无法在(5, +∞)范围内插入数据

-- 其他事务
INSERT INTO t (id) VALUES (7);  -- 被阻塞
INSERT INTO t (id) VALUES (15); -- 被阻塞
```

### 🎓 面试回答模板

```
【ACID基本概念】
- 原子性：事务要么全部完成，要么全部不完成（undo log实现）
- 一致性：事务前后数据保持一致（约束+业务逻辑保证）
- 隔离性：并发事务之间互不干扰（锁+MVCC实现）
- 持久性：提交后数据永久保存（redo log实现）

【隔离级别】
MySQL默认是可重复读（REPEATABLE READ）
- 避免脏读：不能读到未提交的数据
- 避免不可重复读：同一事务内多次读取结果一致
- 部分解决幻读：通过MVCC + Next-Key Lock

【MVCC原理】
- 保存数据的多个版本
- Read View判断数据可见性
- 读不加锁，提高并发性能
- RC每次SELECT生成Read View，RR事务开始时生成
```

---

## 📌 MySQL存储引擎（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **InnoDB** | **MyISAM** | **B+树** | **聚簇索引** | **非聚簇索引** | **事务** | **行锁** | **表锁**

### ✅ InnoDB vs MyISAM

| 特性 | InnoDB | MyISAM |
|------|--------|--------|
| **事务支持** | ✅ 支持 | ❌ 不支持 |
| **外键约束** | ✅ 支持 | ❌ 不支持 |
| **锁粒度** | 行锁 | 表锁 |
| **MVCC** | ✅ 支持 | ❌ 不支持 |
| **崩溃恢复** | ✅ 支持（redo log） | ❌ 不支持 |
| **索引类型** | 聚簇索引 | 非聚簇索引 |
| **存储文件** | .ibd（数据+索引） | .MYD（数据）+ .MYI（索引） |
| **适用场景** | 事务处理、高并发写 | 读多写少、不需要事务 |

### 🔥 面试追问点

#### 1️⃣ 为什么InnoDB是默认引擎？（⭐⭐⭐⭐⭐）

**原因：**
1. **支持事务**：ACID保证数据一致性
2. **行级锁**：并发性能更好
3. **崩溃恢复**：通过redo log保证数据不丢失
4. **外键支持**：保证数据完整性
5. **MVCC**：读写不阻塞，提高并发

**现代应用需求：**
- 金融、电商等对数据一致性要求高
- 高并发场景需要行锁
- 系统可靠性要求高

#### 2️⃣ 聚簇索引和非聚簇索引的区别？（⭐⭐⭐⭐⭐ 高频）

**聚簇索引（Clustered Index）：**
- **定义**：数据行存储在索引的叶子节点中
- **InnoDB的主键索引**就是聚簇索引
- **优点**：查询时直接返回数据，不需要回表
- **缺点**：插入/更新可能导致页分裂

**非聚簇索引（Non-Clustered Index）：**
- **定义**：索引和数据分开存储，叶子节点存储主键值
- **InnoDB的二级索引**就是非聚簇索引
- **优点**：插入/更新效率高
- **缺点**：查询需要回表（先查索引得到主键，再查主键索引得到数据）

**图解：**
```
聚簇索引（主键索引）：
        [10]
       /    \
     [5]    [15]
    /  \    /  \
  [1]  [8] [12] [20]
   ↓    ↓   ↓    ↓
  完整  完整 完整  完整
  数据  数据 数据  数据

非聚簇索引（二级索引）：
        [B]
       /   \
     [A]   [C]
    /  \   /  \
  [A] [B][C] [D]
   ↓   ↓  ↓   ↓
  主键 主键 主键 主键
   5   10  15  20
         ↓
      回表查询主键索引
```

**回表查询：**
```sql
-- 假设有索引：INDEX(name)
SELECT * FROM user WHERE name = '张三';

-- 执行过程：
-- 1. 在name索引树中找到'张三'，得到主键id=10
-- 2. 在主键索引树中根据id=10找到完整数据（回表）
```

**覆盖索引（避免回表）：**
```sql
-- 如果SELECT的列都在索引中，就不需要回表
SELECT id, name FROM user WHERE name = '张三';
-- 因为name索引的叶子节点包含(name, id)，不需要回表
```

---

## 📌 MySQL索引与B+树（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **B+树** | **B树** | **平衡树** | **索引** | **最左前缀** | **联合索引** | **回表** | **覆盖索引**

### ✅ 为什么MySQL使用B+树？

#### 对比其他数据结构

**1. vs 二叉搜索树：**
- ❌ 二叉树可能退化成链表，查询O(n)
- ❌ 树的高度太高，IO次数多

**2. vs 平衡二叉树（AVL/红黑树）：**
- ❌ 每个节点只有2个子节点，树太高
- ❌ 1000万数据，树高度约为24层，需要24次IO

**3. vs B树：**
- ✅ B树是多叉树，树矮胖，IO次数少
- ❌ 但B树的非叶子节点存储数据，导致每个节点存储的key更少

**4. ✅ B+树：**
- ✅ 非叶子节点只存储key，不存储数据
- ✅ 同样大小的节点可以存储更多key，树更矮
- ✅ 叶子节点用链表连接，方便范围查询
- ✅ 所有查询都到叶子节点，性能稳定

### 📊 B+树结构详解

**B+树特点：**
1. **非叶子节点**：只存储key，不存储data
2. **叶子节点**：存储完整数据，用链表连接
3. **所有查询**：都必须到叶子节点
4. **范围查询**：直接遍历叶子节点链表

**结构图：**
```
                  [50]
                 /    \
           [20, 35]    [70, 90]
          /   |   \    /   |   \
      叶子  叶子  叶子  叶子  叶子  叶子
      节点  节点  节点  节点  节点  节点
       ↓     ↓     ↓     ↓     ↓     ↓
    [1-20] [20-35][35-50][50-70][70-90][90-∞]
      ←→    ←→    ←→    ←→    ←→    ←→
    双向链表连接，方便范围查询
```

**为什么B+树一般是3层？**

**计算：**
- InnoDB页大小：16KB
- 主键INT：4字节
- 指针：6字节
- 每个节点可存储key数量：16KB / (4+6) ≈ 1600个

**3层B+树可以存储的数据量：**
```
第1层（根节点）：1600个key
第2层：1600 × 1600 = 256万个key
第3层：1600 × 1600 × 1600 = 40亿条数据

结论：3层B+树可以存储40亿条数据！
IO次数：只需要3次IO就能查到数据
```

### 🔥 面试追问点

#### 1️⃣ B树和B+树的区别？（⭐⭐⭐⭐⭐ 高频）

| 特性 | B树 | B+树 |
|------|-----|------|
| **非叶子节点** | 存储key和data | 只存储key |
| **叶子节点** | 存储key和data | 存储key和data，且有链表 |
| **查询效率** | 不稳定，可能在任意层找到 | 稳定，都要到叶子节点 |
| **范围查询** | 需要中序遍历 | 直接遍历叶子节点链表 |
| **节点存储能力** | 少（因为存了data） | 多（只存key） |

**为什么MySQL选择B+树：**
1. **树更矮**：非叶子节点不存data，可以存更多key
2. **范围查询快**：叶子节点链表，直接遍历
3. **查询稳定**：所有查询都到叶子节点，性能稳定
4. **顺序IO**：叶子节点连续，磁盘预读效率高

#### 2️⃣ 为什么不用哈希表做索引？（⭐⭐⭐⭐⭐）

**哈希表的问题：**
1. ❌ **不支持范围查询**
   ```sql
   -- 哈希表无法高效执行
   SELECT * FROM user WHERE age BETWEEN 20 AND 30;
   ```

2. ❌ **不支持排序**
   ```sql
   -- 哈希表无法利用索引排序
   SELECT * FROM user ORDER BY age;
   ```

3. ❌ **不支持最左前缀匹配**
   ```sql
   -- 哈希表无法利用索引
   SELECT * FROM user WHERE name LIKE '张%';
   ```

4. ❌ **哈希冲突**
   - 当哈希冲突严重时，退化成链表，O(n)

**Memory引擎使用哈希索引：**
- 只支持等值查询
- 速度快，但功能受限

#### 3️⃣ 联合索引的最左前缀原则？（⭐⭐⭐⭐⭐ 高频）

**联合索引：** INDEX(a, b, c)

**索引结构：**
```
按照(a, b, c)的顺序排序：
a | b | c
--|---|---
1 | 1 | 1
1 | 1 | 2
1 | 2 | 1
2 | 1 | 1
2 | 2 | 1
```

**能用到索引的情况：**
```sql
✅ WHERE a = 1                      -- 用到a
✅ WHERE a = 1 AND b = 2            -- 用到a, b
✅ WHERE a = 1 AND b = 2 AND c = 3  -- 用到a, b, c
✅ WHERE a = 1 AND c = 3            -- 用到a（c用不到）

❌ WHERE b = 2                      -- 用不到索引
❌ WHERE c = 3                      -- 用不到索引
❌ WHERE b = 2 AND c = 3            -- 用不到索引
```

**范围查询会中断：**
```sql
WHERE a = 1 AND b > 2 AND c = 3
-- 用到a, b，但c用不到（b使用了范围查询）
```

**MySQL优化器会调整顺序：**
```sql
WHERE b = 2 AND a = 1  -- 优化器会调整为 a = 1 AND b = 2
```

**原理：**
- 索引是按照从左到右的顺序排序的
- 跳过左边的列，后面的列就无法利用索引排序

### 🎓 索引设计建议

**1. 选择合适的列建索引：**
- ✅ WHERE、ORDER BY、GROUP BY的列
- ✅ 区分度高的列
- ✅ 小字段（节省空间）
- ❌ 更新频繁的列
- ❌ 区分度低的列（如性别）

**2. 联合索引设计：**
- 区分度高的列放前面
- 常用的列放前面
- 考虑最左前缀原则

**3. 避免索引失效：**
```sql
❌ SELECT * FROM user WHERE YEAR(create_time) = 2024;  -- 函数导致失效
✅ SELECT * FROM user WHERE create_time >= '2024-01-01' AND create_time < '2025-01-01';

❌ SELECT * FROM user WHERE name LIKE '%张';  -- 前缀模糊导致失效
✅ SELECT * FROM user WHERE name LIKE '张%';

❌ SELECT * FROM user WHERE age = '20';  -- 类型不匹配导致失效
✅ SELECT * FROM user WHERE age = 20;
```

---

## 📌 MySQL日志系统（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **redo log** | **binlog** | **undo log** | **WAL** | **两阶段提交** | **crash-safe** | **主从复制**

### ✅ 三种日志详解

#### 1. redo log（重做日志）

**作用：** 保证事务的**持久性**，实现crash-safe

**特点：**
- **物理日志**：记录"在某个数据页上做了什么修改"
- **InnoDB引擎特有**
- **循环写**：固定大小，写满后从头覆盖
- **WAL技术**：先写日志，再写磁盘

**工作原理：**
```
1. 执行UPDATE语句
2. 先写redo log（顺序IO，快）
3. 将redo log标记为prepare状态
4. 写binlog
5. 将redo log标记为commit状态
6. 后台异步将修改刷入磁盘（随机IO，慢）
```

**crash-safe：**
- 即使数据库崩溃，重启后可以根据redo log恢复数据
- 保证已提交的事务不会丢失

**redo log结构：**
```
[写指针write pos]
  ↓
[检查点checkpoint]
  ↓
redo log buffer → redo log file (4个文件，循环写)
```

#### 2. binlog（归档日志）

**作用：** 用于**数据备份**和**主从复制**

**特点：**
- **逻辑日志**：记录SQL语句的原始逻辑
- **Server层**实现，所有引擎都可以用
- **追加写**：写满一个文件后新开一个文件，不覆盖

**格式：**
1. **STATEMENT**：记录SQL语句
   - 优点：日志量少
   - 缺点：可能导致主从不一致（如NOW()、UUID()）

2. **ROW**：记录每一行数据的变化
   - 优点：主从一致
   - 缺点：日志量大

3. **MIXED**：混合模式，自动选择

**用途：**
1. **主从复制**：slave拉取master的binlog执行
2. **数据恢复**：误删数据可以通过binlog恢复

#### 3. undo log（回滚日志）

**作用：** 保证事务的**原子性**，实现事务**回滚**和**MVCC**

**特点：**
- **逻辑日志**：记录相反的操作
- 例如：INSERT → DELETE，UPDATE → 反向UPDATE

**用途：**
1. **事务回滚**：
   ```sql
   BEGIN;
   UPDATE user SET balance = 100 WHERE id = 1;
   ROLLBACK;  -- 根据undo log执行反向操作
   ```

2. **MVCC**：
   - undo log形成版本链
   - Read View根据版本链判断数据可见性

### 🔥 面试追问点

#### 1️⃣ redo log和binlog的区别？（⭐⭐⭐⭐⭐ 高频）

| 对比项 | redo log | binlog |
|-------|----------|--------|
| **层级** | InnoDB引擎层 | MySQL Server层 |
| **日志类型** | 物理日志（数据页修改） | 逻辑日志（SQL语句） |
| **写入方式** | 循环写（固定大小） | 追加写（不覆盖） |
| **作用** | crash-safe，保证持久性 | 主从复制、数据恢复 |
| **粒度** | 页级别 | 事务级别 |

#### 2️⃣ 两阶段提交是什么？为什么需要？（⭐⭐⭐⭐⭐ 核心）

**问题：** 如果不用两阶段提交，redo log和binlog可能不一致

**场景1：先写redo log，后写binlog，写binlog时崩溃**
```
1. 写redo log成功，标记commit
2. 崩溃，binlog未写入
3. 重启后，根据redo log恢复，数据已修改
4. 但binlog中没有这条记录
5. 主从复制时，slave没有这条记录 → 主从不一致
```

**场景2：先写binlog，后写redo log，写redo log时崩溃**
```
1. 写binlog成功
2. 崩溃，redo log未写入
3. 重启后，数据未修改（redo log中没有）
4. 但binlog中有这条记录
5. 主从复制时，slave有这条记录 → 主从不一致
```

**两阶段提交：**
```
1. 执行UPDATE
2. 写redo log，标记为prepare状态
3. 写binlog
4. 将redo log标记为commit状态

恢复逻辑：
- redo log为prepare，且binlog存在 → 提交事务
- redo log为prepare，但binlog不存在 → 回滚事务
- redo log为commit → 事务已提交
```

**保证：**
- redo log和binlog的一致性
- crash-safe
- 主从数据一致

#### 3️⃣ WAL技术是什么？（⭐⭐⭐⭐）

**WAL（Write-Ahead Logging）：预写式日志**

**原理：**
- 先写日志，再写磁盘
- 日志是顺序IO，速度快
- 数据是随机IO，速度慢
- 后台异步将日志刷入数据页

**好处：**
1. **性能**：顺序IO比随机IO快得多
2. **持久性**：即使崩溃，也可以根据日志恢复
3. **批量写**：日志可以批量刷盘，减少IO次数

**对比：**
```
不使用WAL：
UPDATE → 直接修改数据页（随机IO，慢）

使用WAL：
UPDATE → 写redo log（顺序IO，快）
       → 后台异步刷盘（批量随机IO）
```

### 🎓 面试回答模板

```
【三种日志】
1. redo log（重做日志）：
   - InnoDB引擎层，物理日志
   - 循环写，保证crash-safe
   - 实现事务持久性

2. binlog（归档日志）：
   - Server层，逻辑日志
   - 追加写，用于主从复制和数据恢复
   - 记录所有DDL和DML语句

3. undo log（回滚日志）：
   - 逻辑日志，记录相反操作
   - 实现事务回滚和MVCC

【两阶段提交】
1. 写redo log（prepare）
2. 写binlog
3. 写redo log（commit）

目的：保证redo log和binlog一致性
```

---

## 📌 Redis数据结构（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **String** | **List** | **Hash** | **Set** | **ZSet** | **跳表** | **压缩列表** | **渐进式rehash**

### ✅ Redis 5种基本数据类型

#### 1. String（字符串）

**底层实现：** SDS（Simple Dynamic String）

**特点：**
- 二进制安全，可以存储任何数据
- 预分配空间，减少内存重新分配
- 记录字符串长度，O(1)获取长度

**SDS结构：**
```c
struct sdshdr {
    int len;      // 已使用长度
    int free;     // 剩余空间
    char buf[];   // 字符数组
};
```

**常用命令：**
```bash
SET key value
GET key
INCR key        # 原子性+1
DECR key        # 原子性-1
APPEND key value
```

**应用场景：**
- 缓存：存储对象JSON
- 计数器：文章阅读数、点赞数
- 分布式锁：SETNX
- Session共享

#### 2. List（列表）

**底层实现：**
- Redis 3.2之前：双向链表或压缩列表
- Redis 3.2之后：QuickList（链表 + 压缩列表）

**QuickList：**
- 链表的每个节点是一个压缩列表
- 结合了链表和压缩列表的优点

**常用命令：**
```bash
LPUSH key value    # 左侧插入
RPUSH key value    # 右侧插入
LPOP key           # 左侧弹出
RPOP key           # 右侧弹出
LRANGE key 0 -1    # 获取所有元素
```

**应用场景：**
- 消息队列：LPUSH + BRPOP
- 最新列表：文章列表、评论列表
- 栈：LPUSH + LPOP
- 队列：LPUSH + RPOP

#### 3. Hash（哈希）

**底层实现：**
- **ziplist（压缩列表）**：元素少时使用
- **hashtable（哈希表）**：元素多时使用

**转换条件：**
```
满足以下两个条件之一时，从ziplist转为hashtable：
1. 哈希对象保存的键值对数量 > 512个
2. 键值对的键或值的长度 > 64字节
```

**常用命令：**
```bash
HSET key field value
HGET key field
HMSET key field1 value1 field2 value2
HGETALL key
HDEL key field
```

**应用场景：**
- 存储对象：用户信息、商品信息
- 购物车：key=userId, field=productId, value=count

#### 4. Set（集合）

**底层实现：**
- **intset（整数集合）**：元素都是整数且数量少时
- **hashtable（哈希表）**：其他情况

**常用命令：**
```bash
SADD key member      # 添加元素
SMEMBERS key         # 获取所有元素
SISMEMBER key member # 判断元素是否存在
SINTER key1 key2     # 交集
SUNION key1 key2     # 并集
SDIFF key1 key2      # 差集
```

**应用场景：**
- 标签系统：文章标签、用户兴趣标签
- 共同好友：交集运算
- 推荐系统：差集运算找出未关注的人
- 抽奖系统：SRANDMEMBER随机抽取

#### 5. ZSet（有序集合）

**底层实现：** ⭐⭐⭐⭐⭐ **重点**
- **ziplist（压缩列表）**：元素少时使用
- **skiplist（跳表） + hashtable**：元素多时使用

**跳表（Skip List）详解：**

**结构：**
```
层级4:  1 -----------------> 10
层级3:  1 ------> 5 -------> 10
层级2:  1 -> 3 -> 5 -> 7 --> 10
层级1:  1 -> 3 -> 5 -> 7 -> 8 -> 10  (原始链表)
```

**特点：**
- 多层链表结构
- 每层都是有序链表
- 上层是下层的"快速通道"
- 查找、插入、删除：平均O(logn)

**插入节点时随机层数：**
```c
int randomLevel() {
    int level = 1;
    while (random() < 0.25 && level < MAX_LEVEL) {
        level++;
    }
    return level;
}
```

**为什么用跳表而不用红黑树？**
1. **实现简单**：跳表比红黑树简单
2. **范围查询快**：跳表可以顺序遍历
3. **内存占用**：跳表空间复杂度O(n)，红黑树也是O(n)
4. **并发友好**：跳表更容易实现无锁操作

**常用命令：**
```bash
ZADD key score member
ZRANGE key 0 -1 WITHSCORES  # 按score升序
ZREVRANGE key 0 -1          # 按score降序
ZRANK key member            # 获取排名
ZSCORE key member           # 获取分数
ZINCRBY key increment member
```

**应用场景：**
- 排行榜：游戏排行、热搜排行
- 延时队列：score=时间戳
- 限流：滑动窗口
- 范围查询：按分数范围获取数据

### 🔥 面试追问点

#### 1️⃣ 跳表的查找过程？（⭐⭐⭐⭐⭐）

**例子：** 在跳表中查找7

```
层级3:  1 ------> 5 -------> 10
层级2:  1 -> 3 -> 5 -> 7 --> 10
层级1:  1 -> 3 -> 5 -> 7 -> 8 -> 10

查找7的过程：
1. 从顶层（层级3）的头节点1开始
2. 1的下一个节点是5，5 < 7，继续向右
3. 5的下一个节点是10，10 > 7，下降到层级2
4. 在层级2，5的下一个节点是7，找到！

时间复杂度：O(logn)
```

#### 2️⃣ 压缩列表（ziplist）是什么？（⭐⭐⭐⭐）

**特点：**
- 连续内存空间，节省内存
- 顺序存储，适合小数据量
- 不需要指针，减少内存碎片

**结构：**
```
[zlbytes][zltail][zllen][entry1][entry2]...[entryN][zlend]

zlbytes: 整个ziplist的字节数
zltail: 尾节点的偏移量
zllen: 节点数量
entry: 数据节点
zlend: 结束标记（0xFF）
```

**缺点：**
- 插入/删除可能导致连锁更新（cascade update）
- 数据量大时效率低

**使用场景：**
- Hash、List、ZSet在元素少时使用
- 节省内存

#### 3️⃣ 渐进式rehash是什么？（⭐⭐⭐⭐⭐）

**问题：** 哈希表扩容时，一次性rehash会阻塞服务

**解决：** 渐进式rehash，分多次逐步完成

**过程：**
```
1. 创建新的哈希表ht[1]，大小为ht[0]的2倍
2. 在后续的增删改查操作中，逐步将ht[0]的数据迁移到ht[1]
3. 全部迁移完成后，释放ht[0]，将ht[1]设置为ht[0]

特点：
- 不阻塞服务
- 增删改查时顺便迁移一部分数据
- 查找时需要同时查找ht[0]和ht[1]
- 新增数据直接写入ht[1]
```

**代码逻辑：**
```c
// 查找key
if (在ht[0]中找到) {
    return ht[0][key];
} else if (在ht[1]中找到) {
    return ht[1][key];
}

// 插入key
if (正在rehash) {
    插入到ht[1];
    迁移一小部分数据从ht[0]到ht[1];
} else {
    插入到ht[0];
}
```

### 🎓 面试回答模板

```
【Redis数据结构】
5种基本类型：String, List, Hash, Set, ZSet

【底层实现】
- String: SDS（动态字符串）
- List: QuickList（链表+压缩列表）
- Hash: ziplist或hashtable
- Set: intset或hashtable
- ZSet: ziplist或skiplist+hashtable

【跳表】
- 多层有序链表
- 查找O(logn)，插入删除O(logn)
- 比红黑树简单，范围查询快
- 随机层数，平均空间O(n)

【渐进式rehash】
- 分多次逐步完成扩容
- 不阻塞服务
- 增删改查时顺便迁移数据
```

---

## 📌 Redis持久化（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **RDB** | **AOF** | **快照** | **增量备份** | **AOF重写** | **混合持久化**

### ✅ RDB（Redis Database）

**定义：** 在指定的时间间隔内将内存中的数据集快照写入磁盘

**触发方式：**
1. **SAVE命令**：阻塞Redis服务器，直到RDB完成
2. **BGSAVE命令**：fork子进程，异步执行，不阻塞
3. **自动触发**：配置文件中的save规则

**配置：**
```conf
# 900秒内至少1个key被修改，执行BGSAVE
save 900 1
save 300 10
save 60 10000

# RDB文件名
dbfilename dump.rdb

# 保存路径
dir /var/lib/redis
```

**优点：**
- 文件紧凑，适合备份和灾难恢复
- 恢复速度快
- 性能高（fork子进程）

**缺点：**
- 可能丢失最后一次快照之后的数据
- fork子进程会占用内存
- 数据量大时，fork耗时

**工作原理：**
```
1. Redis fork一个子进程
2. 子进程将数据写入临时RDB文件
3. 写完后，用临时文件替换旧的RDB文件
4. 使用COW（Copy-On-Write）技术，节省内存
```

### ✅ AOF（Append Only File）

**定义：** 将每一个写命令追加到日志文件中

**配置：**
```conf
# 开启AOF
appendonly yes

# AOF文件名
appendfilename "appendonly.aof"

# 同步策略
appendfsync always    # 每个命令都同步，最安全，性能最差
appendfsync everysec  # 每秒同步一次，默认，折中
appendfsync no        # 由操作系统决定，性能最好，可能丢失较多数据
```

**优点：**
- 数据更安全，最多丢失1秒数据
- 可读性好，可以手动修改AOF文件
- 支持自动AOF重写

**缺点：**
- 文件比RDB大
- 恢复速度比RDB慢
- 写入性能比RDB低

### ✅ AOF重写（⭐⭐⭐⭐⭐ 重点）

**问题：** AOF文件会越来越大

**解决：** AOF重写，生成新的AOF文件

**原理：**
```
重写前AOF：
SET name 张三
SET name 李四
SET name 王五
INCR counter
INCR counter
INCR counter

重写后AOF：
SET name 王五
SET counter 3
```

**重写过程：**
```
1. Redis fork子进程
2. 子进程根据内存中的数据生成新的AOF文件
3. 主进程继续处理命令，新命令写入：
   - 旧AOF文件
   - AOF重写缓冲区
4. 子进程完成后，主进程将重写缓冲区的命令追加到新AOF文件
5. 用新AOF文件替换旧AOF文件
```

**触发条件：**
```conf
# AOF文件大小超过上次重写后大小的100%
auto-aof-rewrite-percentage 100

# AOF文件最小大小64MB
auto-aof-rewrite-min-size 64mb
```

### ✅ 混合持久化（Redis 4.0+）

**定义：** 结合RDB和AOF的优点

**原理：**
```
AOF文件结构：
[RDB格式的数据] + [AOF格式的增量命令]

优点：
- 恢复速度快（RDB部分）
- 数据更安全（AOF部分）
```

**配置：**
```conf
aof-use-rdb-preamble yes
```

### 🔥 面试追问点

#### 1️⃣ RDB和AOF如何选择？（⭐⭐⭐⭐⭐）

| 场景 | 选择 | 原因 |
|------|------|------|
| **数据不能丢失** | AOF（everysec） | 最多丢失1秒数据 |
| **可以接受少量数据丢失** | RDB | 性能高，恢复快 |
| **数据量大** | RDB | 文件小，恢复快 |
| **最佳实践** | RDB + AOF | 互补，双保险 |
| **Redis 4.0+** | 混合持久化 | 兼顾性能和安全 |

#### 2️⃣ fork子进程的COW是什么？（⭐⭐⭐⭐）

**COW（Copy-On-Write）：写时复制**

**原理：**
```
1. fork时不复制物理内存，只复制页表
2. 父子进程共享同一份物理内存
3. 当某个进程修改内存时，才复制这一页

好处：
- 节省内存
- fork速度快
```

**示例：**
```
原始内存：10GB

不使用COW：
fork时复制10GB内存 → 内存占用20GB

使用COW：
fork时只复制页表 → 内存占用10GB
修改1GB数据 → 内存占用11GB
```

### 🎓 面试回答模板

```
【RDB】
- 快照，全量备份
- 触发：SAVE、BGSAVE、自动
- 优点：紧凑、恢复快
- 缺点：可能丢失数据

【AOF】
- 记录每个写命令
- 同步策略：always、everysec、no
- 优点：数据安全
- 缺点：文件大、恢复慢

【AOF重写】
- 根据内存数据生成新AOF
- 减小文件大小
- fork子进程，不阻塞

【选择】
推荐RDB+AOF混合持久化：
- 性能：RDB备份
- 安全：AOF防止数据丢失
```

---

## 📌 Redis高可用（⭐⭐⭐⭐⭐ 必考）

### 🎯 得分关键词
> **主从复制** | **哨兵** | **集群** | **读写分离** | **故障转移** | **哈希槽**

### ✅ 主从复制（Replication）

**作用：**
1. 读写分离：master写，slave读
2. 数据备份：slave作为备份
3. 高可用：master挂了，可以提升slave

**配置：**
```conf
# 在slave节点配置
replicaof <master-ip> <master-port>
masterauth <master-password>
```

**全量复制：**
```
1. slave发送PSYNC命令
2. master执行BGSAVE生成RDB
3. master将RDB发送给slave
4. slave清空旧数据，加载RDB
5. master将复制期间的命令发送给slave
```

**增量复制：**
```
1. master记录写命令到复制积压缓冲区
2. slave断线重连后，发送offset
3. master根据offset发送缺失的命令
```

### ✅ 哨兵（Sentinel）

**作用：** 监控master，自动故障转移

**功能：**
1. **监控**：检测master和slave是否正常
2. **通知**：故障时通知管理员
3. **故障转移**：master挂了，自动提升slave
4. **配置中心**：客户端通过哨兵获取master地址

**配置：**
```conf
# 监控master
sentinel monitor mymaster 127.0.0.1 6379 2

# 判断master下线的时间（30秒）
sentinel down-after-milliseconds mymaster 30000

# 故障转移超时时间
sentinel failover-timeout mymaster 180000
```

**故障转移过程：**
```
1. 哨兵检测到master下线
2. 至少2个哨兵确认master下线（主观下线 → 客观下线）
3. 哨兵之间选举，选出一个leader
4. leader从slave中选择一个作为新master
5. 让其他slave复制新master
6. 通知客户端master地址变更
```

**slave选择规则：**
```
1. 排除已下线的slave
2. 排除5秒内没有回复的slave
3. 排除与旧master断开超过10*down-after时间的slave
4. 选择优先级最高的slave（replica-priority）
5. 优先级相同，选择复制偏移量最大的（数据最新）
6. 选择run_id最小的
```

### ✅ Redis Cluster（集群）

**特点：**
- 去中心化，无需proxy
- 数据分片，支持横向扩展
- 高可用，部分节点挂了仍可用

**哈希槽（Hash Slot）：**
```
- 总共16384个槽（0-16383）
- 每个节点负责一部分槽
- 数据根据key的CRC16值分配到槽

槽分配：
节点A：0-5460
节点B：5461-10922
节点C：10923-16383

计算槽：
slot = CRC16(key) % 16384
```

**重定向：**
```
客户端 → 节点A：GET key
节点A：key不在我这，在节点B
       返回：MOVED 5461 节点B地址
客户端 → 节点B：GET key
节点B：返回value

优化：客户端缓存槽和节点的映射关系
```

**故障转移：**
```
1. 节点A挂了
2. 节点A的slave提升为master
3. 集群继续提供服务

如果节点A及其所有slave都挂了：
- 默认：集群不可用
- cluster-require-full-coverage no：部分可用
```

### 🔥 面试追问点

#### 1️⃣ 主从复制的延迟问题？（⭐⭐⭐⭐）

**问题：**
- master写入后，slave可能还没同步
- 客户端从slave读，读到旧数据

**解决方案：**
1. **读主库**：写后立即读主库
2. **延迟读**：写后等待一段时间再读
3. **版本号**：写入时返回版本号，读取时检查版本号
4. **Redis 6.0+**：使用`WAIT`命令等待同步

```bash
# 等待至少2个slave同步，超时1000ms
WAIT 2 1000
```

#### 2️⃣ 哨兵和集群的区别？（⭐⭐⭐⭐⭐）

| 对比项 | 哨兵 | 集群 |
|-------|------|------|
| **架构** | 主从复制 + 哨兵监控 | 去中心化，多主多从 |
| **数据** | 所有节点数据相同 | 数据分片，节点存储不同数据 |
| **扩展性** | 不支持横向扩展 | 支持横向扩展 |
| **高可用** | 自动故障转移 | 自动故障转移 |
| **适用场景** | 数据量小，读多写少 | 数据量大，需要扩展 |

### 🎓 面试回答模板

```
【主从复制】
- master写，slave读
- 全量复制：RDB
- 增量复制：复制积压缓冲区

【哨兵】
- 监控master和slave
- 自动故障转移
- 至少3个哨兵节点

【集群】
- 16384个哈希槽
- 数据分片，支持扩展
- 去中心化，无需proxy
- 故障转移自动化
```

---

## 📌 MongoDB（⭐⭐⭐⭐ 重点）

### 🎯 得分关键词
> **文档数据库** | **NoSQL** | **BSON** | **副本集** | **分片** | **索引**

### ✅ MongoDB基础

**特点：**
- 文档型NoSQL数据库
- 使用BSON（Binary JSON）存储数据
- Schema-free，灵活的数据模型
- 支持复杂查询和聚合
- 水平扩展能力强

**数据模型：**
```javascript
// 关系型数据库
users表：id, name, email
orders表：id, user_id, product_id, amount

// MongoDB
{
  _id: ObjectId("..."),
  name: "张三",
  email: "zhang@example.com",
  orders: [
    {
      product: "手机",
      amount: 3999,
      date: ISODate("2024-01-01")
    },
    {
      product: "电脑",
      amount: 5999,
      date: ISODate("2024-01-02")
    }
  ]
}
```

**优点：**
- 灵活的数据模型，适合快速迭代
- 高性能读写
- 易于扩展
- 丰富的查询功能

**缺点：**
- 不支持事务（4.0+支持多文档事务）
- 占用空间较大
- 不适合复杂的关联查询

### ✅ MongoDB副本集

**作用：** 高可用和数据冗余

**架构：**
```
Primary（主节点）：处理所有写操作
Secondary（从节点）：复制Primary数据，可读
Arbiter（仲裁节点）：只参与选举，不存储数据
```

**故障转移：**
```
1. Primary挂了
2. Secondary之间选举
3. 选出新的Primary
4. 客户端自动重连到新Primary
```

### ✅ MongoDB分片

**作用：** 水平扩展，处理海量数据

**组件：**
- **分片（Shard）**：存储数据的子集
- **配置服务器（Config Server）**：存储元数据
- **路由（Mongos）**：路由查询到对应分片

**分片键：**
```javascript
// 按user_id分片
sh.shardCollection("mydb.users", { user_id: 1 })

// 范围分片
Shard1: user_id [0, 1000)
Shard2: user_id [1000, 2000)
Shard3: user_id [2000, ∞)
```

### 🎓 面试回答模板

```
【MongoDB特点】
- 文档型NoSQL，使用BSON
- Schema-free，灵活
- 支持复杂查询和聚合
- 水平扩展能力强

【副本集】
- Primary处理写操作
- Secondary复制数据，可读
- 自动故障转移

【分片】
- 水平扩展，处理海量数据
- 分片键决定数据分布
- Mongos路由查询

【适用场景】
- 日志系统
- 内容管理系统
- 游戏数据存储
- 物联网数据
```

---

## 📌 数据库设计与优化

### SQL注入防范

**SQL注入示例：**
```sql
-- 恶意输入：' OR '1'='1
SELECT * FROM users WHERE username = '' OR '1'='1' AND password = ''

-- 结果：返回所有用户
```

**防范方法：**
1. **参数化查询**（最重要）
```java
// ❌ 拼接SQL
String sql = "SELECT * FROM users WHERE username = '" + username + "'";

// ✅ 参数化查询
PreparedStatement ps = conn.prepareStatement("SELECT * FROM users WHERE username = ?");
ps.setString(1, username);
```

2. **输入验证**：检查输入是否符合预期格式
3. **最小权限**：数据库账号只给必要的权限
4. **错误信息**：不要暴露数据库详细错误信息

### 索引优化建议

1. **选择合适的列建索引**
2. **避免过多索引**：影响写入性能
3. **使用覆盖索引**：避免回表
4. **定期维护索引**：OPTIMIZE TABLE

---

## 📌 数据库CPU高排查（⭐⭐⭐⭐⭐ 运维必备）

### 🎯 得分关键词
> **慢查询** | **show processlist** | **explain** | **performance_schema** | **bigkey** | **热key** | **索引失效**

---

## 📌 MySQL CPU高排查

### ✅ 排查步骤

#### 第1步：确认CPU高是数据库进程导致

**使用top命令查看：**
```bash
top

# 输出示例：
PID   USER  %CPU  COMMAND
3306  mysql 95.3  mysqld

# 如果mysqld进程CPU占用很高，继续排查
```

**查看MySQL线程CPU占用：**
```bash
# 显示MySQL各线程的CPU使用情况
top -H -p 3306

# 输出示例：
PID    %CPU  COMMAND
12345  45.2  mysqld
12346  30.1  mysqld
12347  20.0  mysqld
```

#### 第2步：查看当前执行的SQL

**使用show processlist：**
```sql
-- 查看所有正在执行的线程
SHOW FULL PROCESSLIST;

-- 输出：
+----+------+-----------+------+---------+------+-------+------------------+
| Id | User | Host      | db   | Command | Time | State | Info             |
+----+------+-----------+------+---------+------+-------+------------------+
| 10 | root | localhost | test | Query   | 120  | Sending data | SELECT * FROM large_table WHERE ... |
| 11 | root | localhost | test | Query   | 95   | Sending data | SELECT * FROM ... |
+----+------+-----------+------+---------+------+-------+------------------+

-- 关键字段：
-- Time: SQL执行时间（秒）
-- State: 当前状态
-- Info: 正在执行的SQL
```

**找出问题SQL：**
```sql
-- 查找执行时间超过10秒的查询
SELECT * FROM information_schema.PROCESSLIST
WHERE COMMAND != 'Sleep'
  AND TIME > 10
ORDER BY TIME DESC;
```

**杀掉慢查询：**
```sql
-- 杀掉指定的查询
KILL QUERY 10;  -- 只杀查询，不断开连接
KILL 10;        -- 杀掉连接
```

#### 第3步：分析慢查询日志

**开启慢查询日志：**
```sql
-- 查看慢查询配置
SHOW VARIABLES LIKE 'slow_query%';
SHOW VARIABLES LIKE 'long_query_time';

-- 开启慢查询日志
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;  -- 超过2秒的查询记录

-- 慢查询日志位置
SHOW VARIABLES LIKE 'slow_query_log_file';
```

**分析慢查询日志：**
```bash
# 使用mysqldumpslow分析
mysqldumpslow -s t -t 10 /var/log/mysql/slow.log

# 参数说明：
# -s t: 按查询时间排序
# -t 10: 显示前10条
# -s c: 按查询次数排序
# -s at: 按平均查询时间排序

# 输出示例：
Count: 1234  Time=5.67s (7000s)  Lock=0.00s (0s)  Rows=10000.0 (12340000)
  SELECT * FROM users WHERE status = 'S'
```

#### 第4步：使用EXPLAIN分析SQL

**EXPLAIN详解：**
```sql
EXPLAIN SELECT * FROM orders WHERE user_id = 100;

-- 输出：
+----+-------------+--------+------+---------------+------+---------+------+------+-------------+
| id | select_type | table  | type | possible_keys | key  | key_len | ref  | rows | Extra       |
+----+-------------+--------+------+---------------+------+---------+------+------+-------------+
|  1 | SIMPLE      | orders | ALL  | NULL          | NULL | NULL    | NULL | 1M   | Using where |
+----+-------------+--------+------+---------------+------+---------+------+------+-------------+
```

**关键字段解读：**

| 字段 | 含义 | 重点关注 |
|------|------|---------|
| **type** | 访问类型 | ALL（全表扫描）❌最差<br>index（索引扫描）<br>range（范围扫描）<br>ref（非唯一索引）<br>eq_ref（唯一索引）<br>const（常量）✅最优 |
| **possible_keys** | 可能使用的索引 | NULL说明没有可用索引 |
| **key** | 实际使用的索引 | NULL说明未使用索引 |
| **rows** | 扫描行数 | 越大性能越差 |
| **Extra** | 额外信息 | **Using filesort**：需要排序❌<br>**Using temporary**：使用临时表❌<br>**Using index**：覆盖索引✅<br>**Using where**：使用WHERE过滤 |

**优化案例：**
```sql
-- ❌ 慢查询（全表扫描）
EXPLAIN SELECT * FROM orders WHERE DATE(create_time) = '2024-01-01';
-- type: ALL, rows: 1000000 ❌

-- ✅ 优化后（使用索引）
EXPLAIN SELECT * FROM orders
WHERE create_time >= '2024-01-01' AND create_time < '2024-01-02';
-- type: range, rows: 100 ✅
```

#### 第5步：Performance Schema分析

**启用Performance Schema：**
```sql
-- 查看是否启用
SHOW VARIABLES LIKE 'performance_schema';

-- 查看耗时最长的SQL
SELECT
    DIGEST_TEXT AS query,
    COUNT_STAR AS exec_count,
    ROUND(AVG_TIMER_WAIT / 1000000000000, 2) AS avg_time_sec,
    ROUND(SUM_TIMER_WAIT / 1000000000000, 2) AS total_time_sec
FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;
```

### 🔥 常见原因和解决方案

#### 1️⃣ 索引缺失或失效（⭐⭐⭐⭐⭐ 最常见）

**现象：**
- EXPLAIN显示type=ALL（全表扫描）
- rows扫描行数很大

**原因：**
```sql
-- 1. 没有建立索引
SELECT * FROM users WHERE email = 'test@example.com';  -- email无索引

-- 2. 索引失效：使用函数
SELECT * FROM users WHERE YEAR(create_time) = 2024;  -- 索引失效

-- 3. 索引失效：前缀模糊匹配
SELECT * FROM users WHERE name LIKE '%张';  -- 索引失效

-- 4. 索引失效：类型不匹配
SELECT * FROM users WHERE age = '20';  -- age是INT，传入字符串

-- 5. 索引失效：OR条件
SELECT * FROM users WHERE name = '张三' OR age = 20;  -- name有索引，age无索引
```

**解决方案：**
```sql
-- 1. 添加索引
CREATE INDEX idx_email ON users(email);

-- 2. 避免使用函数
WHERE create_time >= '2024-01-01' AND create_time < '2025-01-01';

-- 3. 避免前缀模糊
WHERE name LIKE '张%';

-- 4. 类型匹配
WHERE age = 20;

-- 5. OR改为UNION
SELECT * FROM users WHERE name = '张三'
UNION
SELECT * FROM users WHERE age = 20;
```

#### 2️⃣ 大量慢查询堆积（⭐⭐⭐⭐⭐）

**现象：**
- SHOW PROCESSLIST看到大量查询堆积
- 很多查询处于"Sending data"状态

**解决方案：**
```sql
-- 1. 立即杀掉慢查询
SELECT CONCAT('KILL ', id, ';')
FROM information_schema.PROCESSLIST
WHERE TIME > 60 AND COMMAND != 'Sleep';

-- 2. 优化SQL
-- 3. 增加索引
-- 4. 限流：限制并发查询数
SET GLOBAL max_connections = 500;
```

#### 3️⃣ 表锁或行锁等待（⭐⭐⭐⭐）

**现象：**
- SHOW PROCESSLIST看到很多"Waiting for table metadata lock"
- 大量UPDATE/DELETE阻塞

**排查锁等待：**
```sql
-- 查看InnoDB锁等待
SELECT
    r.trx_id waiting_trx_id,
    r.trx_mysql_thread_id waiting_thread,
    r.trx_query waiting_query,
    b.trx_id blocking_trx_id,
    b.trx_mysql_thread_id blocking_thread,
    b.trx_query blocking_query
FROM information_schema.innodb_lock_waits w
INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;
```

**解决方案：**
```sql
-- 1. 找到阻塞的事务，杀掉
KILL 阻塞线程ID;

-- 2. 优化事务：缩小事务范围，快速提交
BEGIN;
UPDATE users SET status = 1 WHERE id = 100;  -- 只锁一行
COMMIT;  -- 立即提交

-- 3. 避免大事务
```

#### 4️⃣ 统计信息过时（⭐⭐⭐⭐）

**现象：**
- EXPLAIN估算的rows不准确
- 优化器选择了错误的索引

**解决方案：**
```sql
-- 更新统计信息
ANALYZE TABLE users;

-- 或者重建表
OPTIMIZE TABLE users;
```

#### 5️⃣ join连接过多或笛卡尔积（⭐⭐⭐⭐⭐）

**现象：**
- EXPLAIN显示rows非常大
- 多表join

**问题SQL：**
```sql
-- ❌ 没有join条件，产生笛卡尔积
SELECT * FROM orders, users;  -- 1万 × 10万 = 10亿行

-- ❌ join条件不当
SELECT * FROM orders o
JOIN users u ON o.user_name = u.name;  -- 字符串join，无索引
```

**解决方案：**
```sql
-- ✅ 使用主键join
SELECT * FROM orders o
JOIN users u ON o.user_id = u.id;  -- 使用索引

-- ✅ 减少join表数量
-- ✅ 使用子查询分步执行
```

### 🎓 MySQL CPU高排查总结

**排查流程：**
```
1. top命令确认是mysqld进程
   ↓
2. SHOW PROCESSLIST找到慢SQL
   ↓
3. EXPLAIN分析执行计划
   ↓
4. 找到原因：索引缺失/失效、表锁、慢查询等
   ↓
5. 优化：加索引、改SQL、杀慢查询
```

**面试回答模板：**
```
【现象】
生产环境MySQL CPU突然飙升到90%+

【排查】
1. top确认是mysqld进程导致
2. SHOW PROCESSLIST找到大量慢查询堆积
3. EXPLAIN分析发现全表扫描，type=ALL
4. 查看发现WHERE条件的字段没有索引

【解决】
1. 立即杀掉部分慢查询，缓解压力
2. 添加索引：CREATE INDEX idx_xxx ON table(column)
3. 优化SQL，避免全表扫描
4. 开启慢查询日志，持续监控

【预防】
1. 上线前做压测和EXPLAIN分析
2. 定期review慢查询日志
3. 建立索引规范和SQL审核机制
```

---

## 📌 Redis CPU高排查

### ✅ 排查步骤

#### 第1步：确认CPU高

**使用top命令：**
```bash
top

# 输出示例：
PID   USER  %CPU  COMMAND
6379  redis 85.3  redis-server
```

#### 第2步：查看Redis慢查询日志

**查看慢查询：**
```bash
# 连接Redis
redis-cli

# 查看慢查询配置
CONFIG GET slowlog-*
# 输出：
# slowlog-log-slower-than: 10000  # 单位：微秒，超过10ms记录
# slowlog-max-len: 128            # 最多保存128条

# 查看慢查询日志
SLOWLOG GET 10

# 输出示例：
1) 1) (integer) 100               # 日志ID
   2) (integer) 1640000000         # 时间戳
   3) (integer) 50000              # 执行时间（微秒）= 50ms
   4) 1) "KEYS"                    # 命令
      2) "user:*"
   5) "127.0.0.1:6379"             # 客户端地址
   6) ""

# 清空慢查询日志
SLOWLOG RESET
```

**设置慢查询阈值：**
```bash
# 设置超过1ms的命令记录
CONFIG SET slowlog-log-slower-than 1000

# 保存到配置文件
CONFIG REWRITE
```

#### 第3步：使用INFO命令分析

**查看Redis状态：**
```bash
INFO

# 关键指标：
# CPU
used_cpu_sys: 85.23           # 系统CPU
used_cpu_user: 120.45         # 用户CPU

# 客户端连接
connected_clients: 1000       # 当前连接数
blocked_clients: 50           # 阻塞的客户端

# 内存
used_memory: 2GB
used_memory_peak: 3GB

# 命令统计
instantaneous_ops_per_sec: 50000  # 当前QPS
```

**查看命令统计：**
```bash
INFO COMMANDSTATS

# 输出示例：
cmdstat_get:calls=10000000,usec=50000000,usec_per_call=5.00
cmdstat_set:calls=5000000,usec=30000000,usec_per_call=6.00
cmdstat_keys:calls=100,usec=10000000,usec_per_call=100000.00  # ❌ 慢命令

# 字段说明：
# calls: 调用次数
# usec: 总耗时（微秒）
# usec_per_call: 平均耗时
```

#### 第4步：使用monitor监控实时命令

**实时监控所有命令：**
```bash
redis-cli MONITOR

# 输出：
1640000000.123456 [0 127.0.0.1:12345] "GET" "user:1000"
1640000000.123789 [0 127.0.0.1:12346] "KEYS" "user:*"  # ❌ 危险命令
1640000000.124000 [0 127.0.0.1:12347] "SET" "user:2000" "data"

# 注意：MONITOR会降低性能，仅用于排查，不要长时间开启
```

#### 第5步：查找大key和热key

**查找大key：**
```bash
# 使用redis-cli扫描大key
redis-cli --bigkeys

# 输出示例：
-------- summary -------
Biggest string: "bigkey1" (10MB)
Biggest list:   "mylist" (100000 items)
Biggest set:    "myset" (50000 members)
Biggest hash:   "myhash" (80000 fields)
Biggest zset:   "myzset" (60000 members)
```

**查找热key（Redis 4.0+）：**
```bash
redis-cli --hotkeys

# 或使用MONITOR + 脚本统计
```

### 🔥 常见原因和解决方案

#### 1️⃣ 慢命令导致CPU高（⭐⭐⭐⭐⭐ 最常见）

**问题命令：**
```bash
# ❌ KEYS命令：扫描所有key，O(n)
KEYS user:*

# ❌ FLUSHALL/FLUSHDB：清空数据库
FLUSHALL

# ❌ SMEMBERS大set：返回所有成员
SMEMBERS large_set  # 100万个元素

# ❌ HGETALL大hash
HGETALL large_hash  # 10万个字段

# ❌ SORT大数据
SORT large_list

# ❌ SUNION/SINTER/SDIFF大集合
SUNION set1 set2 set3  # 每个set 10万元素
```

**解决方案：**
```bash
# ✅ 使用SCAN替代KEYS
SCAN 0 MATCH user:* COUNT 100
# SCAN是渐进式遍历，不会阻塞

# ✅ 使用SSCAN替代SMEMBERS
SSCAN large_set 0 COUNT 100

# ✅ 使用HSCAN替代HGETALL
HSCAN large_hash 0 COUNT 100

# ✅ 限制集合大小
# 设计时避免大集合，拆分为多个小集合

# ✅ 禁用危险命令（配置文件）
rename-command KEYS ""
rename-command FLUSHALL ""
rename-command FLUSHDB ""
```

#### 2️⃣ 大key导致CPU高（⭐⭐⭐⭐⭐）

**现象：**
- 单个key占用内存很大（如string 10MB，list 10万元素）
- 访问大key时CPU飙升

**排查：**
```bash
# 查找大key
redis-cli --bigkeys

# 查看key的内存占用
MEMORY USAGE mykey

# 查看key的类型和元素数量
TYPE mykey
LLEN mylist  # 如果是list
HLEN myhash  # 如果是hash
```

**解决方案：**
```bash
# 1. 拆分大key
# ❌ 单个大hash
HSET user:1000 field1 value1 field2 value2 ... field100000 value100000

# ✅ 拆分为多个小hash
HSET user:1000:0 field1 value1 ...  # 前1000个字段
HSET user:1000:1 field1 value1 ...  # 1000-2000字段

# 2. 删除大key时使用渐进式删除
# ❌ 直接DEL大key（阻塞）
DEL large_key

# ✅ 渐进式删除
# hash/set/zset使用HSCAN/SSCAN/ZSCAN + HDEL/SREM/ZREM
HSCAN large_hash 0 COUNT 100
HDEL large_hash field1 field2 ...

# Redis 4.0+可以使用UNLINK（异步删除）
UNLINK large_key
```

#### 3️⃣ 热key导致CPU高（⭐⭐⭐⭐⭐）

**现象：**
- 某个key被大量访问
- 单个Redis实例CPU高，其他实例正常

**排查：**
```bash
# 使用MONITOR监控
redis-cli MONITOR | grep "user:1000"

# 使用--hotkeys（Redis 4.0+）
redis-cli --hotkeys
```

**解决方案：**
```bash
# 1. 本地缓存
# 应用层使用本地缓存（如Caffeine、Guava Cache）
# 减少Redis访问

# 2. 读写分离
# 热key的读请求分散到多个slave

# 3. 复制多份
# 将热key复制多份，随机访问
SET hot_key:1 value
SET hot_key:2 value
SET hot_key:3 value

# 客户端随机访问
key = "hot_key:" + random(1, 3)

# 4. 使用集群
# 将热key分散到不同的节点
```

#### 4️⃣ 持久化导致CPU高（⭐⭐⭐⭐）

**现象：**
- RDB save或AOF rewrite时CPU飙升
- fork子进程耗时长

**解决方案：**
```bash
# 1. 调整RDB策略
# 配置文件：
save 900 1      # 900秒内至少1个key变化才保存
save 300 10
save 60 10000

# 或者关闭自动RDB，手动执行
save ""

# 2. 调整AOF重写策略
auto-aof-rewrite-percentage 100  # AOF文件增长100%才重写
auto-aof-rewrite-min-size 64mb   # 最小64MB才重写

# 3. 使用混合持久化（Redis 4.0+）
aof-use-rdb-preamble yes

# 4. 避免高峰期持久化
# 在业务低峰期手动执行BGSAVE/BGREWRITEAOF
```

#### 5️⃣ 集群扩容/缩容导致CPU高（⭐⭐⭐⭐）

**现象：**
- Redis Cluster扩容时，数据迁移导致CPU高

**解决方案：**
```bash
# 控制迁移速度
# 配置文件：
cluster-migration-barrier 1  # 至少保留1个slave

# 限流迁移
# 使用redis-cli --cluster reshard时指定慢速迁移
```

### 🎓 Redis CPU高排查总结

**排查流程：**
```
1. top确认是redis-server进程
   ↓
2. SLOWLOG查看慢查询
   ↓
3. INFO COMMANDSTATS查看命令统计
   ↓
4. MONITOR监控实时命令
   ↓
5. --bigkeys查找大key
   ↓
6. 找到原因：慢命令、大key、热key等
   ↓
7. 优化：避免慢命令、拆分大key、本地缓存热key
```

**面试回答模板：**
```
【现象】
Redis CPU突然飙升到90%

【排查】
1. SLOWLOG发现大量KEYS命令
2. INFO COMMANDSTATS确认KEYS调用频繁
3. MONITOR监控发现某个接口频繁调用KEYS user:*

【原因】
代码中使用KEYS命令查找用户，KEYS是O(n)操作，扫描所有key

【解决】
1. 立即下线问题代码
2. 将KEYS改为SCAN，渐进式遍历
3. 在配置中禁用KEYS命令：rename-command KEYS ""
4. 代码review，禁止使用KEYS/FLUSHALL等危险命令

【预防】
1. 建立Redis命令白名单
2. 使用Redis慢查询日志监控
3. 压测时检查CPU使用情况
```

---

## 📌 MongoDB CPU高排查

### ✅ 排查步骤

#### 第1步：确认CPU高

**使用top命令：**
```bash
top

# 输出示例：
PID    USER  %CPU  COMMAND
27017  mongo 88.5  mongod
```

#### 第2步：查看当前操作

**使用db.currentOp()：**
```javascript
// 连接MongoDB
mongo

// 查看当前所有操作
db.currentOp()

// 查看运行时间超过3秒的操作
db.currentOp({"secs_running": {$gt: 3}})

// 输出示例：
{
  "inprog": [
    {
      "opid": 12345,
      "active": true,
      "secs_running": 120,     // 已运行120秒
      "op": "query",           // 操作类型
      "ns": "mydb.users",      // 集合
      "query": {               // 查询条件
        "age": {$gt: 20}
      },
      "planSummary": "COLLSCAN",  // ❌ 全集合扫描
      "numYields": 1000
    }
  ]
}
```

**杀掉慢查询：**
```javascript
// 杀掉指定操作
db.killOp(12345)
```

#### 第3步：查看慢查询日志

**开启慢查询日志：**
```javascript
// 查看当前配置
db.getProfilingLevel()
// 0: 关闭
// 1: 记录慢查询（超过slowms）
// 2: 记录所有查询

// 开启慢查询，记录超过100ms的查询
db.setProfilingLevel(1, 100)

// 查看慢查询
db.system.profile.find().sort({ts: -1}).limit(10)

// 输出示例：
{
  "op": "query",
  "ns": "mydb.users",
  "query": { "age": {$gt: 20} },
  "millis": 5000,        // 执行时间5秒
  "ts": ISODate("..."),
  "planSummary": "COLLSCAN",  // 全集合扫描
  "execStats": {
    "nReturned": 1000,
    "executionTimeMillis": 5000,
    "totalDocsExamined": 1000000  // 扫描了100万文档
  }
}
```

#### 第4步：使用explain分析查询

**EXPLAIN详解：**
```javascript
// 查看执行计划
db.users.find({age: {$gt: 20}}).explain("executionStats")

// 输出：
{
  "executionStats": {
    "executionSuccess": true,
    "nReturned": 1000,              // 返回1000条
    "executionTimeMillis": 5000,    // 耗时5秒
    "totalDocsExamined": 1000000,   // ❌ 扫描100万文档
    "executionStages": {
      "stage": "COLLSCAN",          // ❌ 全集合扫描
      "filter": { "age": {$gt: 20} },
      "nReturned": 1000,
      "executionTimeMillisEstimate": 4800
    }
  }
}
```

**关键指标：**
- **COLLSCAN**：全集合扫描 ❌ 最差
- **IXSCAN**：索引扫描 ✅ 好
- **FETCH**：根据索引获取文档
- **totalDocsExamined**：扫描的文档数，越少越好
- **nReturned**：返回的文档数

**优化后：**
```javascript
// 创建索引
db.users.createIndex({age: 1})

// 再次explain
db.users.find({age: {$gt: 20}}).explain("executionStats")

// 输出：
{
  "executionStats": {
    "nReturned": 1000,
    "executionTimeMillis": 50,     // ✅ 只需50ms
    "totalDocsExamined": 1000,     // ✅ 只扫描1000条
    "executionStages": {
      "stage": "FETCH",
      "inputStage": {
        "stage": "IXSCAN",         // ✅ 使用索引
        "keyPattern": { "age": 1 }
      }
    }
  }
}
```

#### 第5步：查看索引使用情况

**查看索引：**
```javascript
// 查看集合的所有索引
db.users.getIndexes()

// 查看索引使用统计
db.users.aggregate([{$indexStats: {}}])

// 输出：
{
  "name": "age_1",
  "key": { "age": 1 },
  "accesses": {
    "ops": 10000,      // 使用次数
    "since": ISODate("...")
  }
}
```

### 🔥 常见原因和解决方案

#### 1️⃣ 缺少索引导致全集合扫描（⭐⭐⭐⭐⭐ 最常见）

**现象：**
- explain显示COLLSCAN
- totalDocsExamined >> nReturned

**解决方案：**
```javascript
// 1. 创建单字段索引
db.users.createIndex({age: 1})

// 2. 创建复合索引
db.users.createIndex({age: 1, status: 1})

// 3. 查看索引是否生效
db.users.find({age: 20}).explain("executionStats")

// 4. 删除无用索引（占用空间和写入性能）
db.users.dropIndex("old_index")
```

#### 2️⃣ 大量写入导致CPU高（⭐⭐⭐⭐）

**现象：**
- 批量插入/更新时CPU高

**解决方案：**
```javascript
// 1. 使用批量操作
// ❌ 逐条插入
for (let i = 0; i < 10000; i++) {
  db.users.insert({name: "user" + i});
}

// ✅ 批量插入
let bulk = db.users.initializeUnorderedBulkOp();
for (let i = 0; i < 10000; i++) {
  bulk.insert({name: "user" + i});
}
bulk.execute();

// 2. 控制批量大小
// 每次最多1000条
```

#### 3️⃣ 复杂聚合查询（⭐⭐⭐⭐）

**现象：**
- 使用aggregate进行复杂统计时CPU高

**优化方案：**
```javascript
// 1. 在aggregate早期阶段使用$match过滤
// ❌ 先join再过滤
db.orders.aggregate([
  {$lookup: {...}},
  {$match: {status: "paid"}}  // 过滤放在后面
])

// ✅ 先过滤再join
db.orders.aggregate([
  {$match: {status: "paid"}},  // 先过滤
  {$lookup: {...}}
])

// 2. 使用索引
db.orders.createIndex({status: 1})

// 3. 限制返回字段
db.orders.aggregate([
  {$project: {_id: 1, amount: 1}}  // 只返回需要的字段
])
```

### 🎓 MongoDB CPU高排查总结

**排查流程：**
```
1. top确认是mongod进程
   ↓
2. db.currentOp()查看当前操作
   ↓
3. db.system.profile查看慢查询
   ↓
4. explain分析查询计划
   ↓
5. 找到原因：缺少索引、全集合扫描等
   ↓
6. 优化：创建索引、优化查询
```

**面试回答模板：**
```
【现象】
MongoDB CPU飙升到95%

【排查】
1. db.currentOp()发现大量查询在运行
2. db.system.profile查看慢查询日志
3. explain分析发现COLLSCAN（全集合扫描）
4. 查询条件的字段没有索引

【解决】
1. 立即创建索引：db.users.createIndex({age: 1})
2. 验证索引生效：explain显示IXSCAN
3. 查询时间从5秒降到50ms
4. CPU降到正常水平

【预防】
1. 上线前explain分析所有查询
2. 开启慢查询日志监控
3. 定期review索引使用情况
```

---

## 📌 数据库CPU高排查总结

### 通用排查思路

| 步骤 | MySQL | Redis | MongoDB |
|------|-------|-------|---------|
| **1. 确认进程** | `top` 查看mysqld | `top` 查看redis-server | `top` 查看mongod |
| **2. 查看当前操作** | `SHOW PROCESSLIST` | `SLOWLOG` | `db.currentOp()` |
| **3. 分析慢查询** | 慢查询日志 | `SLOWLOG GET` | `db.system.profile` |
| **4. 执行计划** | `EXPLAIN` | - | `explain()` |
| **5. 找到原因** | 索引缺失、表锁 | 慢命令、大key | 全集合扫描 |
| **6. 优化** | 加索引、改SQL | 避免慢命令、拆分key | 创建索引 |

### 高频面试问题

**Q：如果生产环境数据库CPU突然飙升到90%，你会怎么排查？**

**A（STAR法则回答）：**
```
【Situation】
生产环境MySQL CPU突然从20%飙升到90%，影响业务

【Task】
需要快速定位问题并解决

【Action】
1. 使用top确认是mysqld进程导致
2. 执行SHOW PROCESSLIST，发现大量查询堆积
3. 发现某个查询执行时间超过60秒，状态为Sending data
4. EXPLAIN分析发现type=ALL，全表扫描100万行
5. 检查发现WHERE条件的字段没有索引

【Result】
1. 立即杀掉部分慢查询：KILL QUERY xxx
2. 添加索引：CREATE INDEX idx_xxx ON table(column)
3. CPU降到正常水平，业务恢复
4. 后续优化：review所有SQL，补全缺失索引

【预防】
1. 建立SQL审核机制，上线前必须EXPLAIN
2. 监控慢查询日志，及时优化
3. 定期进行压力测试
```

这样你就全面掌握了数据库CPU高的排查方法！
